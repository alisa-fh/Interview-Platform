\documentclass[12pt,a4paper]{article}
\usepackage{times}
\usepackage{durhampaper}
\usepackage{harvard}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tabularx}


\citationmode{abbr}
\bibliographystyle{agsm}

\title{Using Etiquette Strategies to Mitigate Negative Emotions in Computer-Based Job Interviews}
\author{} % leave; your name goes into \student{}
\student{Ali'sa-Falaq Hussain}
\supervisor{Suncica Hadzidedic}
\degree{BSc Computer Science}

\date{}

\begin{document}

\maketitle

\begin{abstract}
	
{\bf Context/Background -} Different machine learning techniques have been investigated to identify affect in users in order that technology can adapt to optimise user experience. There has been limited research conducted regarding adapting a user interface in a computer interview context, particularly that reducing negative emotions which could impede candidate performance.

{\bf Aims -} The aim of this study is to investigate the effects of feedback on user emotion through the means of facial expression and speech analysis with convolutional neural network models. Particularly, it evaluates the use of human-automation etiquette strategies to minimise negative emotion of candidates in interviews.

{\bf Method -}

{\bf Results -}

{\bf Conclusions -}


The abstract must be a Structured Abstract with the headings {\bf Context/Background}, {\bf Aims}, {\bf Method}, {\bf Results}, and {\bf Conclusions}.  This section should not be longer than half of a page, and having no more than one or two sentences under each heading is advised.
\end{abstract}

\begin{keywords}
Put a few keywords here.
\end{keywords}

\section{Introduction}
Human computer interaction (HCI) is a multidisciplinary field of study which focuses on the design of computer technology and the interaction between humans and computers, with a focus on bridging the gap between users and the system \cite{pantic2003toward}. Affective computing is a specific area of HCI referring to computing that relates to, arises from and deliberately influences emotion \cite{picard2000affective}. Applications in this field address a user's affect, also known as their feeling, emotion or mood \cite{picard2000affective}. Affect is recognised and the system adapts in response to improve the user's experience \cite{klein2002computer}. Picard emphasises emotion's critical role in every computer-related thought or action \cite{picard2000affective}, reinforcing the importance of this field. For instance frustration impacts productivity, learning and creativity \cite{amsel1992frustration}. The majority of existing research focuses on the recognition of emotional states and their impacts on tasks \cite{giannopoulos2018deep} \cite{ceaparu2004determining}. Few have worked on adaptations to mitigate emotions that may impede task performance \cite{yang2015effect}. This is a critical part of research since systems which aim to mitigate emotions, particularly frustration, demonstrate an improvement in responsiveness, engagement \cite{huber2016recognizing}, task success \cite{wang2008politeness} and motivation \cite{aist2002experimentally}. 

Affective computing has been studied rigorously in fields such as education to improve engagement in students \cite{woolf2009affect}. However affect is underexplored in other areas, such as computer automated job interviews. This domain is becoming increasingly important with the rise of automated job interviews, accelerated by the Covid-19 pandemic \cite{WhyUseaV18:online}. We take the opportunity to exploit a user interface to mitigate a candidate's emotional states underlying frustration that could impede task performance and consequently confound judgement on their suitability for a job. We aim to achieve this through objectives outlined in subsequent subsections.

\subsection{Affect Recognition}
Central to affective computing is the recognition of emotional states so that the system can adapt appropriately to optimise user experience. Emotion recognition can be achieved using various features such as face \cite{huber2016recognizing}, speech \cite{petrushin1999emotion}, skin conductivity \cite{woolf2009affect} and text \cite{klein2002computer}. Equipment necessary can vary from expensive specialist equipment \cite{cohen2003evaluation} to basic webcam and microphones \cite{barron2012intelligent}. Facial expression analysis is the most popular \cite{giannopoulos2018deep}, a large factor being the availability of datasets. A common model of emotions used across research is Ekman's six basic emotions: fear, anger, disgust, sadness, joy and surprise \cite{ekman1992argument}. Using this model is advantageous because of the extensive work done to develop methods for recognising the emotions \cite{giannopoulos2018deep}.Additionally, the six emotions can be combined to form more complex ones, including frustration \cite{ekman1992argument}. This model forms the basis of many emotion datasets validated by research such as the open-source FER-2013 facial expression dataset. An alternate model is the Facial Action Coding System (FACS), used to code movements of specific groups of muscles also known as action units. Although more precise due to its region-based approach \cite{ekman2002facial}, publicly available labelled datasets are difficult to source. For this reason we focus on the former approach.


Fewer developments have been made with speech emotion recognition. Existing work tends to focus on methods for feature extraction and analysis. A recent approach, called multi-task learning uses song to supplement speech data. However, it is difficult to know how to best group these two types of data for stable, optimal outputs. \cite{zhang2016cross}. The older RELIEF-F algorithm \cite{kononenko1994estimating} is commonly used to extract prosodic information including pitch, duration and intensity of utterances. This has been implemented in contexts such as call centres to automatically understand emotional states and prioritise voice messages \cite{petrushin1999emotion}. The simplicity and stability of this approach makes it appropriate for a basic interview platform. Again, little research has been done in the application of this information to mitigate negative emotions.

Historically, the use of Bayesian networks for affect classification was popular, because of their ability to handle missing data during inference and training \cite{cohen2003learning}. This method was shown to produce up to 72.5\% accuracy, an improvement on other methods such as Tree-Augmented Naive Bayes. However more recently convolutional neural networks (CNNs) have been used more with advantages such as a simpler architecture, faster training and boasting up to 96.7\% accuracy. In addition, it proves suitable for real time applications \cite{LOPES2017610}, such as real time analysis in an interview platform.

\subsection{System Adaptations}
A further aspect of affective computing is system adaptation. 	fqs are taxonomised into four key categories: function allocation, such as the offloading of tasks; task scheduling which entails timings and duration of tasks; content including quality and quantity; and interaction such as style and interface features \cite{feigh2012toward}. Research, particularly in education, is heavy on function allocation and content adaptations. This often means selecting tasks that engage learners most, and presenting it in a particular way. Our work focusses on interaction style, investigating whether this alone can influence a user's emotional state. Particularly, we transfer the concept of etiquette in human-human interaction to an HCI context. We investigate if computer systems can adapt communication style, in the way a human would, to minimise frustration, and improve task performance. Studies on the effect of etiquette between humans indicate improved communication and encourages effective tutoring. Implementing similar strategies into computers should influence user trust and system performance \cite{parasuraman2004trust}. From this emerges the research question: can etiquette strategies prevalent in human interaction be applied by a computer to mitigate negative user emotions in a computer-based automated job interview?

\subsection{Objectives}
To address the research question, we split the study into three sets of objectives: minimum, intermediate and advanced.

The minimum objective works towards a basic user system and the collection of one type of data. The system is a graphical user interface through which a set of predetermined questions would be asked for users to respond to as part of an interview. It would have to resemble existing platforms to ensue representative results. Facial expression data would be collected, with the use of real-time expression recognition integrated into the application. CNNs would be used to develop a facial expression recognition model. In response to recognised emotional states, the system should use hard coded rules to make appropriate comments.

Intermediate objectives introduces speech as the secondary data to be analysed in our multimodal system. CNNs would be used to train a model on vocal data and corresponding emotional states. Hard coded rules determine outputted feedback in response to recognised negative emotional states. 

Finally, the advanced objectives looks to train the facial expression model on a particular user. Facial and speech data would both be used to understand the change in negative emotional state after the computer's feedback. Finally, the effect of etiquette strategies on a user's overall task performance would also be analysed.

* To include a couple sentences on what was achieved after experiment is completed *



\section{Related Work}
[ Data indicates anxious interview candidates are less likely to be hired, despite this anxiety not correlating with job performance as judged by a supervisor \cite{schneider2019does}.Anxiety and frustration have been shown to have a detrimental impact on a system user. An example of this is in a digital symbol substitution test \cite{burgess1964effects}, where frustration hindered user performance. From an educational perspective, frustration diminishes student motivation \cite{weiner1985attributional}, attention \cite{niedenthal1994emotional} and creativity \cite{isen1987positive}. ]
\subsection{Affect Recognition}
The determination of affect is often done by analysing multiple data sources. Facial expression analysis is popular due to its unobtrusive nature. Different expression recognition approaches have been developed recently, improving on performance. Deep learning developments is particularly key to this progress \cite{voulodimos2018deep}. In 2003, Ira et al tested different Bayesian networks classifiers on emotion classification in videos, proposing a Hidden Markov Model architecture \cite{cohen2003evaluation}. Work has progressed since then to a more recent focus on CNNs \cite{cohen2003learning}. This is due to increase in data available for training neural networks, and advances in GPU technology for low-cost, high performance numerical calculations \cite{LOPES2017610}. Lopes et al looked at combining CNNs with specific pre-processing steps such as artificial rotations and translations, producing accuracies of 96.76\% on the Cohn Kanade+ dataset \cite{lucey2010extended}. This work demonstrated how pre-processing images can overcome the need for controlled environments and issues related to a lack of data. This approach also produced speeds which allowed for real time recognition on standard computers, important for the application in this work.

The RELIEF-F algorithm \cite{kononenko1994estimating} is commonly used in speech analysis to extract vocal features. This was used in the study of vocal emotion expression in call centres to optimise how calls were prioritised \cite{petrushin1999emotion}. 700 utterances were analysed, with features such as pitch, speaking rate, formants and energy rate extracted and analysed. Evaluating a range of machine learning methods, the most accurate was an ensemble of an odd number of neural network classifiers, each trained on a subset of the training set, making an overall decision using the majority voting principle. Although results were impressive, more recent work supports the use of CNNs, whose structures such as local connectivity and weight sharing make it better suited to dealing with environment and speaker variations, reducing the error rate.
\subsection{Frustration in HCI}
Frustration is defined as an emotional state appearing when obstacles block the possibility of achieving a goal \cite{lawson1965frustration}. It is one of the most common experiences in HCI \cite{ceaparu2004determining}, often interfering with the completion of tasks \cite{waterhouse1953frustration}. An example of such was an operator's task performance in a robot vehicle teleoperating task \cite{yang2015effect}. Face tracking and electrodermal activity indicated a decrease in user satisfaction when there was a delay in system response. Studies have looked into how to account for negative affect in the development of systems. In a driving context, a study looked to mitigate frustration with the use of Ambient Light Patterns \cite{locken2017towards}. Woolf et al worked on a tutor system that would encourage the completion of student's tasks despite frustration \cite{woolf2009affect}. This was achieved by mirroring student actions to show empathy, adjusting the authority level of the tutoring system to ease pressure, and changing the voice, motion and gestures of the avatar. The sensing and responding to emotions such as frustation is a step towards building a human-like affective computer \cite{klein2002computer}. We consider if responsive actions such as these could produce a human-like affective computer that can mitigate negative emotions and consequently improve task performance in interviews. 
\subsection{Adaptations}
Easy with Eve is an affective tutoring system for primary school mathematics which uses a lifelike animated agent to adapt to students. The six basic facial expressions as defined by Ekman \cite{ekman1992argument} is used, as is applied in our study. In response to displayed expressions, the agent, Eve, responds with facial expressions and gives feedback, according to pre-defined cases. Barron et al also followed Ekman's model to propose an intelligent tutoring system (ITS) \cite{barron2012intelligent}, which combined information on the user's affect as determined by facial expression and speech data, with personal and academic information to present exercises to students. A dynamic decision network is used to determine the tutor's best course of action, depending on learning and affect utility measures. Our study also provides affect-dependent feedback in a similar manner to this ITS, but with the specifc aim of minimising negative emotion. Autotutor is another ITS which extends student feedback to a conversation to optimise learning. Additional affective states are also considered, with the help of 5 trained judges including confusion, boredom and flow. Although these states may appear relevant to our study, we did not have access to trained judges.
\subsection{Etiquette Strategies}
Etiquette strategies between humans were first defined by Brown and Levinson as ways to formulate messages in order to save the hearer's positive face when face-threatening acts (FTAs) are inevitable \cite{brown1987politeness}. FTAs are acts that damage the face of the addressee by acting in oposition of the desires of the other. The strategies are rules guiding conversation used to facilitate cooperation to maintain each other's face. There are four types of etiquette strategies: bald, negative politeness, positive politeness and off-record. Bald takes no account into the level of imposition to the hearer, and does not attempt to mimise threat to the hearer's face. An example of a bald statement is, "answer the question". Positive politeness minimises imposition between the speaker and hearer with statements of solidarity and compliments. An example is, "you look sad, can I do anything?". The concept of etiquette has been applied to automation, to make interactions between humans and computers natural and polite. A virtual manufacturing plant training system evaluated direct and indirect interactions towards students, and found indirect communications elicited higher student motivation \cite{qu2005using}. In terms of learning efficiency, positive and negative politeness strategies have both shown benefits when using a factory training system \cite{johnson2010role}. It is clear that particular contexts suit different politeness strategies. A hospital information system was evaluated in terms of politeness and appropriateness \cite{bickmore2010response}. Ratings were highest in bald, positive and negative politeness conditions. Off-record was reported as less subtle and considerate. Extending this, a study aiming to mitigate student frustation in a tutoring system showed that the most effective strategy on a student's motivation, confidence, satisfaction and performance varied dependent on whether the student was frustrated \cite{yang2018evaluating}. Additionally, positive and negative politeness improved confidence and satisfaction. From this, we look at positive politeness and bald strategies to evaluate the effect on user frustration and task performance.
\subsection{Summary}
Frustration is very common in HCI \cite{ceaparu2004determining}, particularly affecting task performance, which is an issue when computers arguably exist to optimise work. The recognition and consequent adaptation to a user's affect to mitigate negative emotions underpinning frustration could improve the interaction between systems and humans, consequently improving task performance. Although many studies focus on developing affect recognition methods, investigating the best ways to extract facial or vocal features for recognition, and the effect of factors on affect, fewer investigate adaptations to mitigate negative affective states. Etiquette strategies provide a way for computers to address affect in a natural way resembling human interaction. This study aims to apply the recognition methods discussed to extend Yang and Dorneich's work on etiquette strategies in tutoring \cite{yang2018evaluating} to determine whether these strategies can be used to mitigate negative emotions underpinning frustration in computer interviews. This in turn should optimise task performance and allow accurate judgements to be made on a candidate's job suitability.


\section{Solution}
In this section we present a solution to investigate the extent to which etiquette strategies mitigate negative user emotional states in a computer-automated job interview. To begin, a solution architecture outline is presented before an in-depth discussion of the solution, tools used and the underlying algorithms in the subsequent subsections.

[There exists a range of means to recognise affect. Multimodal systems, which detect emotional states in more than one way are standard in research for their increased reliability \cite{jaimes2007multimodal}. Prior studies have made use of specialist equipment to measure physiological signals such as skin conductance and heart rate \cite{pantic2003toward}, which are arguably more objective, and provide accurate measures for a user's mental workload. However, required equipment is reported to be expensive and obtrusive \cite{tao2019systematic}. Alternatively, Woolf et al found non-obtrusive measures including facial expression analysis effective and placed users in a more natural setting \cite{woolf2009affect}. Similarly, evaluating speech with a microphone in call centres was also unnoticable, and achieved an average accuracy of 77\%. These advantages can be transferred to an interview context, where speech and facial data would naturally be collected.]
\subsection{Solution Architecture Overview}
An overview of the solution is illustrated in Figure 1. We divide this into the basic system through which the user interacts, \textbf{a}, and the models that feed into this, creeated with components \textbf{b, c} and \textbf{d} of the solution. Key to the basic system is a simple GUI. Through this the questions can be accessed, which users respond to using their microphone and webcam. A set of bald and positive politeness style comments are established and outputted in response to negative affect. The resulting emotional states are recorded.

The facial expression dataset, FER-2013 \cite{Goodfeli-et-al-2013}, and speech dataset, RAVDESS, \cite{livingstone2018ryerson} used in component \textbf{b} and \textbf{d} respectively are acquired as discussed in the following subsections. After data preparation, audio data is augmented to overcome issues related to environment variations. Two CNN architectures are developed using Keras, a high-level API for Tensorflow, an open source software library for machine learning. Training and testing of the models are important to ensure optimal parameter configuration. The application uses both models simultaneously to recognise the real-time affective state of the user. 

 The personalised facial expression module, \textbf{c}, involves training the model on the individual participant, who may express emotions in a specific way. Stimulus images are presented to the participant, to obtain their natural reactions. Captured and labelled facial expressions are appended to the original FER-2013 dataset for a more tailored model.

The application was implemented in Python because of the extensive Tensorflow and OpenCV documentation for Python. Additionally the PySimpleGUI library for designing graphical interfaces is robust and well-documented.

\begin{figure}[h]
	\centerline{\includegraphics[scale=.3]{diagrams/solutionoverview.png}}
	\caption{Solution overview}
\end{figure}

\subsection{Basic System}
\subsection*{Interview}
The interview structure should resemble a computer automated job interview normally faced by university graduates in terms of style, length and difficulty for reliable, representative results. To establish graduate level difficulty, questions were sourced from the Graduate Records Examinations (GRE), a standardised test used for graduate school entry in the United States and Canada. There exists a vast amount of available data on the GRE, including the rates of correct answers. It was also used by Yang and Dorneich in their study on mitigating frustration in education \cite{yang2018evaluating}.  Although they used the mathematics component of the exam, we decided on verbal reasoning, because of its suitability for discussion in an interview and students studying a mathematical-based degree would not have an advantage. Since the GRE leaves an average of 1.5 minutes per question, this was the standard time allocated per question in the interview. HireVue, an automated computer interview platform that have hosted over 20 million interviews, reported that the average interview has between 5 and 8 questions \cite{WhyUseaV18:online}. As a result, we decided on 6 questions. The interview would haved a 50\% split between easy and hard questions, similar to the GRE.

Frustration is defined as the emotional state present when obstacles impair the progress towards achieving a goal \cite{lawson1965frustration}. To evaluate the system's ability to mitigate frustration, we must first induce it. Previous studies have achieved this by changing the description of a problem's difficulty and imposing a time constraint \cite{yang2018evaluating}. Since a difference between perceived and actual difficulty can cause frustration \cite{hone2006empathic}, we mislabelled the hardest question, with the lowest percentage of correct answers as "easy". Additionally, we reduce the time allocated to this question to 50 seconds, along with one other question, resulting in two frustration-inducing questions, 1/3 of the exam.

\subsection*{GUI}
The principal tools used in the design of the interface were PySimpleGUI and OpenCV. Existing computer interview software were analysed to determine the main features: the question, total time, time left and a screen of what the webcam was recording. For frustration inducing purposes, we also required a label indicating the difficulty of the question. See figure 2a and 2b for an example of HireVue's interface and our system respectively. The interface is comprised of PySimpleGUI components. OpenCV is used to collect frames from the webcam in real-time. These are then displayed by refreshing the window every millisecond with the latest frame. The information available before clicking start and beginning the timer include the overall task written at the top, time allocated and question difficulty. It is important this information is shared before to ensure it is read, potentially contributing towards frustration. The user discusses their thought process to the camera and selects their final answers using checkboxes before the time runs out and the checkboxes are disabled. The timer turns red to alert the user when 10 seconds remain, contributing towards the time-constraint aspect of frustration. When a negative emotional state is consistently recognised using methods that will be detailed in further sections, feedback is outputted in red, just below the image of themselves so it is easily visible. The user is presented with results at the end, rather than after each question so to not confound with emotions during the exercise. The Python Shelve library allows the saving of a persistant database file the user can access and send for analysis. Comprehensive instructions are provided to the user before system use.


\begin{figure}
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.95\linewidth]{diagrams/hirevue-interface.png}
		\caption{Hirevue interface}
		\label{fig:sub1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.95\linewidth]{diagrams/my-interface.png}
		\caption{Our interface}
		\label{fig:sub2}
	\end{subfigure}
	\caption{Existing application Hirevue alongside our application}
	\label{fig:test}
\end{figure}
\subsection*{System Feedback}
As discussed, etiquette strategies in human-human interaction has been investigated and more recently applied to human-computer interaction. Observations consistently show that positive politeness and bald communication encourages students struggling to solve problems and improves confidence and satisfaction \cite{yang2018evaluating}. We investigate whether these two strategies effectively mitigate frustration in our interview. When negative emotions are flagged by facial expressions, the output rotates between no feedback, bald and positive politeness. The initial is to establish a baseline against which to compare effects of the strategies. The comments used are outlined in Table 1. Recall a bald strategy takes no account into the level of imposition to the hearer. An example from the original paper \cite{brown1987politeness} is "pass the hammer". We achieve this with the use of command words such as "consider". For positive politeness we aim to display an element of solidarity. An example from the original paper is "I'll just come along, if you don't mind." To minimise social distance from the hearer, we make use of the third person with words such as "we" and "let's".
	
	\begin{table}[h]
		\centering
		\caption{Feedback Comments}
		\begin{tabularx}{\linewidth}{|l|>{\raggedright\arraybackslash}X|}
			\hline
	 {\bf Bald} & {\bf Positive Politeness} \\ \hline
		Consider each potential word definition carefully. & We can do this, why don’t we consider each word’s definition? \\ \hline
		Ensure you have read the entire text slowly. & Gettting there, let’s make sure we’ve read through the text slowly! \\ \hline
		Consider which words make the text flow best. & Shall we see which words seem most similar?\\ \hline
		Make sure the whole text is understood. &  Shall we take it one word at a time?\\ \hline
		Consider which words sound best in the context. & Let’s see if we can figure which words fit best. \\ \hline
		Consider which words are most similar. & Good concentration! We can try eliminating unlikely words.\\ \hline
\end{tabularx}
\end{table}
\subsection{Convolutional Neural Networks}
There are various existing methods used for the classification of facial expressions and speech audio, such as Bayesian networks because of their ability to handle missing data during inference and training \cite{cohen2003learning}. For facial expressions, this method was shown to produce up to 72.5\% accuracy. However more recently convolutional neural networks (CNNs) have been used more with advantages such as a simpler architecture, faster training and boasting up to 96.7\% accuracy. \cite{LOPES2017610}. For speech analysis, the RAVDESS \cite{livingstone2018ryerson} dataset was tested against various models, where CNN proved most accurate, followed by Random Forest, SVM and finally decision tree \cite{christy2020multimodal}. 

A thorough discussion of the underlying theory of CNNs is beyond the scope of this work, but an overview is given for an understanding of the decisions made in this work. A network is composed of an input, output and hidden layers. These are composed of perceptrons, each which taken an input and apply an activation function to produce an output, fed into the next perceptron over a weighted edge. Supervised learning is where data labelled with the expected output (e.g. anger) is inputted into the network. The output is computed, and the network adjusted by changing connection weights to improve classification. This is referred to as learning. Artificial neural networks (ANNs) are used for pattern recognition and classification but are not capable of extracting image features. CNNs extend this by using a combination of convolutional and pooling layers to extract features from the images. They reduce the number of parameters required to process an image by performing a convolution operation on images so that perceptrons can be arranged in three dimensions, width, height and depth. Images can be efficiently processed this way.

The main objective of the convolution layer is to extract high level features from the image, the number of layers depending on the complexity of the image. It uses different weighted kernels used to extract image features. These are slid over the input image, and computing a dot product of the filter with pixel values, features are calculated for different pixels based on their neighbouring pixel values. The CNN learns the weight of the kernels. This operation results in a two dimensional activation, giving the response of the filter at each pixel value in the image. Activation functions are used to introduce non linearity to the network by deciding which perceptrons should be activated. ReLU, rectified linear unit, is an example of this which allows faster training of data and is also able to eliminate the problem of the vanishing gradient problem, where the gradient tends to 0 as x tends to +$\infty$ or - $\infty$.

The purpose of the pooling layer is to reduce the computational complexity required to process the volume of data associated with an image. This is achieved by splitting the activation maps into grids the size of a pooling kernel and applying a non-linear downsampling to each section. Max pooling returns the maximum value of each portion of the image.

After pooling, a flatten function is applied to the output, converting it to a tabular structure for classification by the ANN. A dropout layer is used to prevent overfitting by reducing the correlation between neurons. A selection of activation maps are ignored during training, although all are used during testing. Finally, in a dense layer, each neuron receives an input from all neurons in the previous layer. Using this we can shape the output of the network to the number of classes in the problem.

\subsection{Modality I - Facial Expression}
Key tools used in the facial expression recognition module include Keras, the high-level API for tensorflow, an open source library for machine learning, the FER-2013 dataset \cite{giannopoulos2018deep} and the open source computer vision library OpenCV. Google Colaboratory was used for model training because of its access to CUDA, providing GPUs for processing.

Our selected dataset would ideally contain complex emotions such as boredom, flow and eureka as used in a previous ITS \cite{barron2012intelligent}. However, relevant datasets were either not publicly accessible or were too small for deep learning purposes. The previous ITS also made use of trained judges, beyond our available resources. As applied in and validated by several previous studies
\cite{woolf2009affect}, we decided on a dataset based on Ekman's six basic emotions \cite{ekman1992argument}, FER-2013. Factors in play towards this decision included dataset size, date of dataset creation, and their use in research. FER-2013 is well-validated \cite{giannopoulos2018deep} and is composed of 28,709 images (figure 3) labelled angry, disgust, fear, surprise, happy, sad and neutral.  Frustation, although not directly identifiable in this dataset can be identified through its constituent emotions. According to the classic valence-arousal dimensions of emotion \cite{hepach2011conceptualizing}, frustration, was classed in the same category as angry, sadness, disgust and fear, having similar valence and arousal values. the degree of positive or negative affective response that the image evokes, along with arousal ratings, the intensity of the affective response that the image evokes Therefore, we focus on minimising these underlying negative emotions.

\begin{figure}[h]
	\centerline{\includegraphics[scale=.2]{diagrams/fer2013.png}}
	\caption{A selection of images from the FER-2013 dataset}
\end{figure}

The images are sized 48x48 and are grayscale. The singular colour channel, as opposed to having 3 reduces complexity and simplifes the problem so it is less computationally expensive. Data is received as a CSV file, with an integer emotion label between 0 and 6 and 2304 pixel values (0-255) for each image. Pixel values were rescaled by 1/255 for more manageable processing. Data was split into training and test sets with rations 75\% and 25\% respectively. Finally, we convert the labels to 2-dimensional one-hot encoded arrays.

The final architecture is drawn in figure 4. The four blue rectangles are the convolutional layers. Max pooling layers are implemented after the second third and final convolutional layer. The flatten and fully connected layers are in brown and yellow respectively. The activation function used is ReLU, \[ y = max(0,x) \] since it is not computationally expensive to run and is sparsely activated. The max function means a perceptron is only activated for a positive input. Increased sparsity often means concise models with less overfitting and better predicting power. Similarly the dropout layers randomly select nodes to be set to 0, also preventing overfitting. The dropout probability for the first two are 0.25, and the final is set to 0.5. We ran the model over 250 epochs several times, adjusting parameters each time, reaching an optimal accuracy of 76\% * under a learning rate of 0.0001 and decay of $1 \times 10^{-6}$ with the Adam optimizer algorithm. The final model is exported and applied to the application.

\begin{figure}[h]
	\centerline{\includegraphics[scale=.2]{diagrams/facialcnn.png}}
	\caption{CNN architecture diagram}
\end{figure}


Detection of the user's face is done using a Haar feature-based cascade classifier \cite{viola2001rapid}. This is an object detection method that is trained using a lot of positive and negative images, so those with and without faces respectively, to train the classifier. We used a pre-trained one provided by OpenCV which had been validated in research, with better recall values than alternate existing classifiers such as Pico \cite{kalinovskii2015compact}. When a face is detected in a webcam frame, the image is converted to grayscale and resized to 48x48 as required by our model. This is run through the model, outputting a two dimensional array, with each index representing a different emotion. Values in the array sum to one, representing confidence in each emotion. If the overall probability of negative emotions (anger, fear, digust, sadness) outweighed positive emotions (happy, neutral, surprise) we classed the frame as negative, and vice versa. 

We also incorporated a module for tailoring the model towards a specific user. The original dataset is extended with images of the user to produce a new model. This requires images of the user naturally displaying each emotional state. Research have used means such as visual stimuli, music and autobigraphical recall to induce emotion. The most effective \cite{siedlecka2019experimental} method has been reported as visual. Chosen visual stimuli were taken from the Open Affective Standardized Image Set (OASIS) dataset \cite{kurdi2017introducing}. These images come labelled with valence and arousal ratings derived from an online study of 822 participants. To choose images for each affective state, we refer to the valence and arousal ratings of each emotional state \cite{buechel2016emotion}. A graphical representation is shown in figure 5. For each affective state bar "surprise" we selected one image that had been rated similarly to the values determined by Buechel and Hahn and placed these in a GUI. A sample of images selected are presented in figure 6. Surprise was eliminated since studies have historically struggled to induce the state \cite{siedlecka2019experimental} so images collected would likely confound the model. Between each stimulus image, the user is presented with a low arousal rated image, "resetting" them to a neutral expression. Once a user is exposed to the image, every second frame is recorded, processed and saved to file. Processing the image converts it into the format of FER-2013 images: the frame is converted to grayscale and resized to 48 x 48. We combine this data with original image data, and retrain the model. Figure 7 shows examples of processed expression images.

\begin{figure}[h]
	\centerline{\includegraphics[scale=.5]{diagrams/emotionratings.png}}
	\caption{Valence and arousal ratings of the 7 chosen emotions}
\end{figure}

\begin{figure}[h]
	\centerline{\includegraphics[scale=.5]{diagrams/stimulus-examples.png}}
	\caption{Examples of stimulus images taken from the OASIS dataset}
\end{figure}

\begin{figure}[h]
	\centerline{\includegraphics[scale=.5]{diagrams/expressions-collected.png}}
	\caption{Examples of expressions collected}
\end{figure}


\subsection{Modality II - Voice}
Voice emotion recognition begins with the preparation of a dataset for training.We utilise the RAVDESS dataset \cite{livingstone2018ryerson} which contains 7356 audio (speech and song) and visual recordings of 12 male and 12 female actors pronouncing English sentences with expressions in the categories calm, happy, sad, angry, fearful, surprise, and disgust. The dataset has been used extensively in research, for applications such as gender recognition and for the evaluation of machine learning models such as multitask learning \cite{zhang2016cross}. This dataset was selected for its availability, research validation and for the emotions available which matched  FER-2013. We narrowed data down to speech, of which there were 1440 files. Each clip is around 4 seconds, with the first and final second usually silence.

Preliminary to training is data preparation. Silence from the clip was trimmed. We relabelled the dataset so that calm, happy and surprise were classed as 'positive', and sad, angry, disgust were 'negative'. The python audio processing library, Librosa, was used to extract Mel Frequency Cepstral Co-efficients (MFCCs). These are state-of-the-art features used widely to recognise speech. On the surface, the conventional frequency is converted to Mel Scale, which takes into account human perception of frequencies. It scales the frequency to match more closely what the human ear can hear. A graphical representation of MFCC coefficients for a fearful audio clip is shown in figure 9.

\begin{figure}[h]
	\centerline{\includegraphics[scale=.3]{diagrams/sample-mfcc.png}}
	\caption{Fearful speech MFCC diagram}
\end{figure}

For a model more resistant to noise, we augment prosodic features in the noise files by stretching the sound, shifting the pitch and adding white noise. This is also a means of increasing the dataset, in itself improving the model. We develop a CNN, referring to Issa et al's study on the use of CNNs for emotion recognition \cite{ISSA2020101894}. Their proposed framework obtained a 71.61\% accuracy on the RAVDESS dataset. As shown in the diagram, the model is composed of one-dimensional convolutional layers combined with batch normalisation, dropout and activation layers. The input layer is composed of 256 filters with a kernel size of 8. It receives 259x1 number arrays. Similar to the facial expression CNN atchitecture, the output is activated by ReLU. After another convolutional layer, batch normalisation is applied, activated by ReLU and followed by dropout with a 0.25 rate. This output is fed into a max-pooling layer with a window size of 8. 4 convolution layers follow this with 128 filters of size 8, the first three activated by ReLU layers, and the final being followed by batch normalisation, a dropout of rate 0.25 and a final max pooling layer. After two further convolution and ReLU pairs, the output is flattened, and received by a fully connected layer with 2 units. These outputs are normalised by a softmax activation layer, converting them from weighted sum values to probabilities that sum to one. These represent the probability of membership for each class, positive and negative.

\begin{figure}[h]
	\centerline{\includegraphics[scale=.7]{diagrams/audiocnn.png}}
	\caption{Speech CNN diagram}
\end{figure}

We access the user's microphone and record clips, which are fed into the model in real-time.. A "clip" refers to a continuous speech input with no pauses. When the application detects silence the current clip is ended and analysed. 

While the user is responding to questions, facial expressions and voice must be processed concurrently. Since Python is naturally synchronous, it was necessary to implement some form of multiprocessing or multithreading. Threads are quicker to spawn than processes, and are in the same memory space so it is easier to share objects, while processes are kept separate. A shared object was necessary to keep track of recognised negative affect. A flag is set either after 8 consecutive frames containing a negative expression, or 3 consecutive negative speech clips. When a flag is set, a feedback comment is outputted for 5 seconds, and the emotional state probabilities from each frame and clip is recorded for this time period, which would then be analysed to determine the impact of this etiquette strategy on emotional state.

Development followed a waterfall lifecycle, with some iterative elements. Agile methodologies such as Scrum was not deemed necessary, since they are more apt for team projects with specific stakeholders for whom the system is being designed. For example, as a solo developer creating a software for experiment purposes, stand-ups and regular stakeholder feedback were not possible. As commonly used in agile methodologies, a Kanban board was designed using Microsoft Planner to visualise the structure of our project and deliverable timelines. The design phase was broken down into user interface design and system design. The former included similar system research, and UI sketches. Additionally, a user-perspective process diagram was drawn. System design focussed on CNN architecture and process communication, particularly between vocal and facial analysis modules. In the verification phase we investigate whether the system has been implemented correctly. The final system was compared against each initial requirement. Ideally a code walkthrough or inspection would be undertaken by another developer. Instead, after a weekend away from the code, I would return and search for consistency and performance improvements to improve reliability and maintainability of the system. I did this at three separate points during and after development. This also resulted in refinement of system requirements, as some features seemed not sufficiently specified. This introduced an iterative element to development. Due to the unpredictability of user behaviour and the complexities of our multimodal system, rigorous testing was necessary. Different black box unit and integration test cases were drawn up from both a usability and system execution perspective. White box testing was required to check the internal functioning of the system, verifying different paths and conditions, such as the behaviour of the negative feedback flag in response to different inputs.


\subsection{Figures and Tables}
It should be noted that not all the details of the work carried out in the project can be represented in 20 pages.  It is therefore vital that the Project Log book be kept up to date as this will be used as supplementary material when the project paper is marked.  There should be between 10 and 20 referenced papers---references to Web based pages should be less than 10\%.

In general, figures and tables should not appear before they are cited.  Place figure captions below the figures; place table titles above the tables.  If your figure has two parts, for example, include the labels ``(a)'' and ``(b)'' as part of the artwork.  Please verify that figures and tables you mention in the text actually exist.  make sure that all tables and figures are numbered as shown in Table \ref{units} and Figure 1.
%sort out your own preferred means of inserting figures

\begin{table}[htb]
	\centering
	\caption{UNITS FOR MAGNETIC PROPERTIES}
	\vspace*{6pt}
	\label{units}
	\begin{tabular}{ccc}\hline\hline
		Symbol & Quantity & Conversion from Gaussian \\ \hline
	\end{tabular}
\end{table}

\subsection{References}

The list of cited references should appear at the end of the report, ordered alphabetically by the surnames of the first authors.  References cited in the main text should use Harvard (author, date) format.  When citing a section in a book, please give the relevant page numbers, as in \cite[p293]{budgen}.  When citing, where there are either one or two authors, use the names, but if there are more than two, give the first one and use ``et al.'' as in  , except where this would be ambiguous, in which case use all author names.

You need to give all authors' names in each reference.  Do not use ``et al.'' unless there are more than five authors.  Papers that have not been published should be cited as ``unpublished''.  Papers that have been submitted or accepted for publication should be cited as ``submitted for publication'' as in .  You can also cite using just the year when the author's name appears in the text, as in ``but according to Futher \citeyear{futher}, we \dots''.  Where an authors has more than one publication in a year, add `a', `b' etc. after the year.

\section{Solution}

This section presents the solutions to the problems in detail.  The design and implementation details should all be placed in this section.  You may create a number of subsections, each focussing on one issue.  

This section should be between 4 to 7 pages in length.

\section{Results}

this section presents the results of the solutions.  It should include information on experimental settings.  The results should demonstrate the claimed benefits/disadvantages of the proposed solutions.

This section should be between 2 to 3 pages in length.

\section{Evaluation}
limitations: only 2 standard sentences in audio dataset and each clip is small, only works for english speaking people.8

This section should between 1 to 2 pages in length.

\section{Conclusions}

This section summarises the main points of this paper.  Do not replicate the abstract as the conclusion.  A conclusion might elaborate on the importance of the work or suggest applications and extensions.  This section should be no more than 1 page in length.

The page lengths given for each section are indicative and will vary from project to project but should not exceed the upper limit.  A summary is shown in Table \ref{summary}.

\begin{table}[htb]
	\centering
	\caption{SUMMARY OF PAGE LENGTHS FOR SECTIONS}
	\vspace*{6pt}
	\label{summary}
	\begin{tabular}{|ll|c|} \hline
		& \multicolumn{1}{c|}{\bf Section} & {\bf Number of Pages} \\ \hline
		I. & Introduction & 2--3 \\ \hline
		II. & Related Work & 2--3 \\ \hline
		III. & Solution & 4--7 \\ \hline
		IV. & Results & 2--3 \\ \hline
		V. & Evaluation & 1-2 \\ \hline
		VI. & Conclusions & 1 \\ \hline
	\end{tabular}
\end{table}


\bibliography{projectpaper}


\end{document}